{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "SEED=200\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar uma das duas funções abaixo:\n",
    "## Reescalar entre 0 e 1 ou Normalização z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(df, columns, maximum=None, minimum=None):\n",
    "    \"\"\"\n",
    "    df: dataframe com o dataset\n",
    "    columns: nomes das colunas que devem ser reescaladas\n",
    "    maximum: dicionário com os valores maximos, com cada chave representando uma coluna\n",
    "    minimum: dicionário com os valores minimos, com cada chave representando uma coluna\n",
    "    retorna o dataset reescalado\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"implementar a função rescale\")\n",
    "\n",
    "def normalize(df, columns, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    df: dataframe com o dataset\n",
    "    columns: nomes das colunas que devem ser normalizadas\n",
    "    mean: dicionário com os valores médios, com cada chave representando uma coluna\n",
    "    std: dicionário com os desvios padrão, com cada chave representando uma coluna\n",
    "    retorna o dataset normalizado\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"implementar a função normalize\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A célula seguinte carrega o dataset. Não é necessário modificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load dataset\"\"\"\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df_full = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                       columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "def get_train_test_inds(y, train_proportion=0.7):\n",
    "    \"\"\"\n",
    "    y: coluna do atributo alvo\n",
    "    retorna os indices de treino e teste estratificados pela classe\n",
    "    \"\"\"\n",
    "    y=np.array(y)\n",
    "    train_inds = np.zeros(len(y),dtype=bool)\n",
    "    test_inds = np.zeros(len(y),dtype=bool)\n",
    "    values = np.unique(y)\n",
    "    for value in values:\n",
    "        value_inds = np.nonzero(y==value)[0]\n",
    "        np.random.shuffle(value_inds)\n",
    "        n = int(train_proportion*len(value_inds))\n",
    "\n",
    "        train_inds[value_inds[:n]]=True\n",
    "        test_inds[value_inds[n:]]=True\n",
    "\n",
    "    return train_inds,test_inds\n",
    "\n",
    "\n",
    "train_inds, test_inds = get_train_test_inds(df_full.loc[:, \"target\"])\n",
    "df_full[['target']] = df_full[['target']].astype(int)\n",
    "df = df_full[train_inds]\n",
    "df_val = df_full[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use este trecho no caso de optar por reescalar\"\"\"\n",
    "df, maximum, minimum = rescale(df, df.columns[0:4])\n",
    "df_val, _, _ = rescale(df_val, df_val.columns[0:4], maximum, minimum)\n",
    "\n",
    "\"\"\"Use este trecho no caso de optar por normalizar\"\"\"\n",
    "# df, mean, std = normalize(df, df.columns[0:4])\n",
    "# df_val, _, _ = normalize(df_val, df_val.columns[0:4], mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Ativação e Softmax\n",
    "## Implementar Relu, Sigmoid e Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(object):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada (batch_size, numero_de_neuronios)\n",
    "        retorna a saída da ativação relu (batch_size, numero de neuronios)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"implementar o relu\")\n",
    "    \n",
    "    def backward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada no backward (batch_size, numero_de_neuronios)\n",
    "        retorna a saída da derivada da ativação relu (batch_size, numero_de_neuronios)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"implementar o relu\")\n",
    "\n",
    "        \n",
    "class Sigmoid(object):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada (batch_size, numero_de_neuronios)\n",
    "        retorna a saída da ativação sigmoidal (batch_size, numero_de_neuronios)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"implementar o sigmoid\")\n",
    "        \n",
    "    def backward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada no backward (batch_size, numero_de_neuronios)\n",
    "        retorna a saída da derivada da ativação sigmoidal (batch_size, numero_de_neuronios)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"implementar o sigmoid\")\n",
    "        \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: Entrada (batch_size, numero_de_neuronios)\n",
    "    retorna a probabilidade de cada classe (batch_size, numero_de_neuronios)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"implementar o softmax\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A célula seguinte implementa uma Rede Neural com 2 camadas escondidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size,\n",
    "                 activation_fn='relu', learning_rate=1e-2, sigma=1., weight_decay=0.1):\n",
    "        \"\"\"Inicializacao pronta. Nao alterar\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # relu or sigmoid\n",
    "        if activation_fn == 'relu':\n",
    "            self.activation_fn = Relu()\n",
    "        elif activation_fn == 'sigmoid':\n",
    "            self.activation_fn = Sigmoid()\n",
    "        \n",
    "        # inicializa as matriz dos parametros\n",
    "        self.params = {}\n",
    "        self.params[\"w1\"] = sigma * np.random.randn(input_size, hidden_size1)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size1)\n",
    "        self.params[\"w2\"] = sigma * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size2)\n",
    "        self.params[\"w3\"] = sigma * np.random.randn(hidden_size2, output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "        # inicializa as matrizes de gradientes\n",
    "        self.grads = {}\n",
    "        self.grads[\"w1\"] = np.zeros((input_size, hidden_size1))\n",
    "        self.grads[\"b1\"] = np.zeros(hidden_size1)\n",
    "        self.grads[\"w2\"] = np.zeros((hidden_size1, hidden_size2))\n",
    "        self.grads[\"b2\"] = np.zeros(hidden_size2)\n",
    "        self.grads[\"w3\"] = np.zeros((hidden_size2, output_size))\n",
    "        self.grads[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: entrada da rede neural\n",
    "        retorna as probabilidades de cada classe (batch_size, numero_de_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"implementar o forward\")\n",
    "        \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        y: indices das classes esperadas (batch_size, 1)\n",
    "        retorna o loss e o dicionario de gradientes\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"implementar o backward\")        \n",
    "        \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Faz um step do gradiente descendente, nao alterar\n",
    "        \"\"\"\n",
    "        self.params[\"w3\"] = self.params[\"w3\"] - self.learning_rate*(self.grads['w3'])\n",
    "        self.params[\"b3\"] = self.params[\"b3\"] - self.learning_rate*(self.grads['b3'])\n",
    "\n",
    "        self.params[\"w2\"] = self.params[\"w2\"] - self.learning_rate*(self.grads['w2'])\n",
    "        self.params[\"b2\"] = self.params[\"b2\"] - self.learning_rate*(self.grads['b2'])\n",
    "        \n",
    "        self.params[\"w1\"] = self.params[\"w1\"] - self.learning_rate*(self.grads['w1'])\n",
    "        self.params[\"b1\"] = self.params[\"b1\"] - self.learning_rate*(self.grads['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devem ser alterados os hiperparâmetros para melhor otimização da rede\n",
    "## Alguns dos hiperparâmetros que podem ser modificados estão na célula abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "activation_fn = 'relu'\n",
    "weight_decay = 0.001 # lambda\n",
    "epochs = 1000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.loc[:,\"target\"].unique() # numero de classes do dataset\n",
    "classes.sort()\n",
    "\n",
    "network = NeuralNetwork(df.shape[1]-1, 10, 10, output_size=len(classes), learning_rate=learning_rate,\n",
    "                        activation_fn=activation_fn, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Nao alterar esta celula\"\"\"\n",
    "# Cria o grafico e atualiza os pesos\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8,3))\n",
    "ax1.set_title('Training Loss')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax3.set_title('Validation Accuracy')\n",
    "\n",
    "t = tqdm(range(epochs))\n",
    "losses = []\n",
    "training_accuracy = []\n",
    "accuracy = []\n",
    "\n",
    "# Visualizacao do treinamento\n",
    "drawn, = ax1.plot(np.arange(0), losses, c='r')\n",
    "drawn2, = ax2.plot(np.arange(0), training_accuracy, c='r')\n",
    "drawn3, = ax3.plot(np.arange(0), accuracy, c='r')\n",
    "\n",
    "# Loop de treinamento\n",
    "for e in t:\n",
    "    # aleatoriza o treino e zera estatisticas\n",
    "    df = df.sample(frac=1., random_state=SEED)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Loop da epoca\n",
    "    for i in range(0, df.shape[0]-batch_size, batch_size):\n",
    "        total += batch_size\n",
    "        batch = df.iloc[i:(i+batch_size), :]\n",
    "        y = df.iloc[i:(i+batch_size)]\n",
    "        y = y.loc[:, \"target\"]\n",
    "        batch = batch.drop([\"target\"], axis=1)\n",
    "        predictions = network.forward(batch.values)\n",
    "        loss, grads = network.backward(y)\n",
    "        total_loss += loss\n",
    "        network.optimize()\n",
    "        correct += sum(y.values == predictions.argmax(axis=1))\n",
    "    \n",
    "    # Atualiza estatisticas e graficos\n",
    "    training_accuracy.append(correct/total)\n",
    "    losses.append(total_loss)\n",
    "    t.set_description('Loss: %.3f' % total_loss)\n",
    "    drawn.set_data((np.arange(len(losses)), losses))\n",
    "    ax1.relim()\n",
    "    ax1.set_xlim((0, e))\n",
    "    ax1.autoscale_view()\n",
    "    \n",
    "    drawn2.set_data((np.arange(len(training_accuracy)), training_accuracy))\n",
    "    ax2.relim()\n",
    "    ax2.set_xlim((0, e))\n",
    "    ax2.autoscale_view()\n",
    "    \n",
    "    ### VALIDACAO ###\n",
    "    batch = df_val\n",
    "    y = df_val.loc[:, \"target\"]\n",
    "    batch = batch.drop([\"target\"], axis=1)\n",
    "\n",
    "    predictions = network.forward(batch.values)\n",
    "    correct = np.sum(y.values == predictions.argmax(axis=1))\n",
    "    accuracy.append(correct/df_val.shape[0])\n",
    "    \n",
    "    # Atualiza grafico de validacao\n",
    "    drawn3.set_data((np.arange(len(accuracy)), accuracy))\n",
    "    ax3.relim()\n",
    "    ax3.set_xlim((0, e))\n",
    "    ax3.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
