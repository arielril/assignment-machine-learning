{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "SEED=200\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar uma das duas funções abaixo:\n",
    "## Reescalar entre 0 e 1 ou Normalização z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(df, columns, maximum=None, minimum=None):\n",
    "    \"\"\"\n",
    "    df: dataframe com o dataset\n",
    "    columns: nomes das colunas que devem ser reescaladas\n",
    "    maximum: dicionário com os valores maximos, com cada chave representando uma coluna\n",
    "    minimum: dicionário com os valores minimos, com cada chave representando uma coluna\n",
    "    retorna o dataset reescalado\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"implementar a função rescale\")\n",
    "\n",
    "def normalize(df, columns, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    df: dataframe com o dataset\n",
    "    columns: nomes das colunas que devem ser normalizadas\n",
    "    mean: dicionário com os valores médios, com cada chave representando uma coluna\n",
    "    std: dicionário com os desvios padrão, com cada chave representando uma coluna\n",
    "    retorna o dataset normalizado\n",
    "    \"\"\"\n",
    "    if mean and std:\n",
    "        for c in columns:\n",
    "            df[c] = df[c].apply(lambda x: ((x - mean[c]) / std[c]))\n",
    "        return df, mean, std\n",
    "    \n",
    "    # normalize each defined column \n",
    "    for c in columns:\n",
    "        if not mean:\n",
    "            mean = {}\n",
    "        if not std:\n",
    "            std = {}\n",
    "        mean[c] = mean[c] if c in mean else df[c].mean()\n",
    "        std[c] = std[c] if c in std else df[c].std()\n",
    "    return df, mean, std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A célula seguinte carrega o dataset. Não é necessário modificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Load dataset\"\"\"\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df_full = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                       columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "def get_train_test_inds(y, train_proportion=0.7):\n",
    "    \"\"\"\n",
    "    y: coluna do atributo alvo\n",
    "    retorna os indices de treino e teste estratificados pela classe\n",
    "    \"\"\"\n",
    "    y=np.array(y)\n",
    "    train_inds = np.zeros(len(y),dtype=bool)\n",
    "    test_inds = np.zeros(len(y),dtype=bool)\n",
    "    values = np.unique(y)\n",
    "    for value in values:\n",
    "        value_inds = np.nonzero(y==value)[0]\n",
    "        np.random.shuffle(value_inds)\n",
    "        n = int(train_proportion*len(value_inds))\n",
    "\n",
    "        train_inds[value_inds[:n]]=True\n",
    "        test_inds[value_inds[n:]]=True\n",
    "\n",
    "    return train_inds,test_inds\n",
    "\n",
    "\n",
    "train_inds, test_inds = get_train_test_inds(df_full.loc[:, \"target\"])\n",
    "df_full[['target']] = df_full[['target']].astype(int)\n",
    "df = df_full[train_inds]\n",
    "df_val = df_full[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Use este trecho no caso de optar por reescalar\"\"\"\n",
    "# df, maximum, minimum = rescale(df, df.columns[0:4])\n",
    "# df_val, _, _ = rescale(df_val, df_val.columns[0:4], maximum, minimum)\n",
    "\n",
    "\"\"\"Use este trecho no caso de optar por normalizar\"\"\"\n",
    "df, mean, std = normalize(df, df.columns[0:4])\n",
    "df_val, _, _ = normalize(df_val, df_val.columns[0:4], mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Ativação e Softmax\n",
    "## Implementar Relu, Sigmoid e Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(object):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada (batch_size, numero_de_neuronios) (10x4) => input layer size == 4 (neuron number)\n",
    "        retorna a saída da ativação ReLu (batch_size, numero_de_neuronios) (10x4) \n",
    "            => output layer size == 4 (neuron number)\n",
    "        \n",
    "        Function: returns the max between the value(x) and 0 (zero)\n",
    "        \"\"\"\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada no backward (batch_size, numero_de_neuronios)\n",
    "        retorna a saída da derivada da ativação relu (batch_size, numero_de_neuronios)\n",
    "        \n",
    "        Function: \n",
    "            - if the value(x) is grater than 0, set the value(x) to 1\n",
    "            - otherwise, set the value(x) to 0\n",
    "        \"\"\"\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "        \n",
    "class Sigmoid(object):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada (batch_size, numero_de_neuronios) (NxM)\n",
    "        retorna a saída da ativação sigmoidal (batch_size, numero_de_neuronios) (NxM) \n",
    "            => output layer size == 4 (neuron number)\n",
    "        \n",
    "        Function: squashes the values in the range [0, 1]\n",
    "            -> base sigmoid\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "        \n",
    "    def backward(self, x):\n",
    "        \"\"\"\n",
    "        x: Entrada no backward (batch_size, numero_de_neuronios) (NxM)\n",
    "        retorna a saída da derivada da ativação sigmoidal (batch_size, numero_de_neuronios) (NxM)\n",
    "        \n",
    "        Function: set the value(x) in the range of [0, .25]\n",
    "            -> sigmoid derivative\n",
    "        \"\"\"\n",
    "        return (1. - x) * x\n",
    "        \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    APPLY TO THE LAST HIDDEN LAYER \n",
    "    \n",
    "    x: Entrada (batch_size, numero_de_neuronios) (NxM)\n",
    "    retorna a probabilidade de cada classe (batch_size, numero_de_neuronios) (NxM)\n",
    "    \"\"\"\n",
    "    exps = np.exp(x - np.amax(x, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A célula seguinte implementa uma Rede Neural com 2 camadas escondidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    PARAMS:\n",
    "    - input layer size => 4 (neurons, 4x1)\n",
    "    - hidden layers:\n",
    "        - h1 size => 10 (neurons => vertical vector)\n",
    "        - h2 size => 10 (neurons => vertical vector)\n",
    "    - output layer size => 3 (neurons, 3x1)\n",
    "    - learning rate => 1.e-3\n",
    "    - sigma => 1.\n",
    "    - activation fn => 'relu' or 'sigmoid'\n",
    "    - weight decay => 0.0001\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size,\n",
    "                 activation_fn='relu', learning_rate=1e-2, sigma=1., weight_decay=0.1):\n",
    "        \"\"\"Inicializacao pronta. Nao alterar\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # relu or sigmoid\n",
    "        if activation_fn == 'relu':\n",
    "            self.activation_fn = Relu()\n",
    "        elif activation_fn == 'sigmoid':\n",
    "            self.activation_fn = Sigmoid()\n",
    "        \n",
    "        \"\"\"\n",
    "        WEIGHTS and BIASES:\n",
    "            - w1 (4x10)\n",
    "            - b1 (10x)\n",
    "            ------------\n",
    "            - w2 (10x10)\n",
    "            - b2 (10x)\n",
    "            -----------\n",
    "            - w3 (10x3)\n",
    "            - b3 (3x)\n",
    "            -----------\n",
    "        \"\"\"\n",
    "        \n",
    "        # inicializa as matrizes dos parametros, init with random values\n",
    "        self.params = {}\n",
    "        self.params[\"w1\"] = sigma * np.random.randn(input_size, hidden_size1)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size1)\n",
    "        self.params[\"w2\"] = sigma * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size2)\n",
    "        self.params[\"w3\"] = sigma * np.random.randn(hidden_size2, output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "        # inicializa as matrizes de gradientes, init with 0\n",
    "        self.grads = {}\n",
    "        self.grads[\"w1\"] = np.zeros((input_size, hidden_size1))\n",
    "        self.grads[\"b1\"] = np.zeros(hidden_size1)\n",
    "        self.grads[\"w2\"] = np.zeros((hidden_size1, hidden_size2))\n",
    "        self.grads[\"b2\"] = np.zeros(hidden_size2)\n",
    "        self.grads[\"w3\"] = np.zeros((hidden_size2, output_size))\n",
    "        self.grads[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "        # init cache\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: entrada da rede neural (bx4) => (batch size, input layer size)\n",
    "        retorna as probabilidades de cada classe (batch_size, numero_de_classes) (bx3)\n",
    "        \n",
    "        Steps:\n",
    "            1. compute the first layer\n",
    "            2. compute the second layer, with the result of the first layer...\n",
    "            3. apply softmax function with the result of the second layer and return\n",
    "        \"\"\"\n",
    "        active = self.activation_fn.forward\n",
    "        \n",
    "        # z1 => mult part between input and first hidden layer\n",
    "        # 10x4 dot 4x10 + 10xM => out 10x10 (fea, batch)\n",
    "        z1 = x.dot(self.params['w1']) + self.params['b1']\n",
    "        # a1 first hidden layer (4x10)\n",
    "        a1 = active(z1)\n",
    "        \n",
    "        # z2 => multiplication between first hidden layer and second hidden layer\n",
    "        # 10x10 dot 10x10 + 10xM => out 10x10\n",
    "        z2 = a1.dot(self.params['w2']) + self.params['b2']\n",
    "        # a2 second hidden layer\n",
    "        a2 = active(z2)\n",
    "        \n",
    "        # z3 => multiplication between second hidden layer and output layer\n",
    "        # 10x10 10x3 + 3xM => out 10x3\n",
    "        z3 = a2.dot(self.params['w3']) + self.params['b3']\n",
    "        # a3 output layer\n",
    "        a3 = softmax(z3)\n",
    "        \n",
    "        self.cache['a1'] = a1\n",
    "        self.cache['a2'] = a2\n",
    "        self.cache['a3'] = a3\n",
    "        \n",
    "        # application of softmax to output layer\n",
    "        return a3\n",
    "        \n",
    "    def compute_decay(self, weight):\n",
    "        return np.sum(np.square(weight))        \n",
    "        \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        y: indices das classes esperadas (batch_size, 1) (bxM) => target column\n",
    "        retorna o loss e o dicionario de gradientes (loss, grads) => (Real number, dict)\n",
    "        \n",
    "        Steps:\n",
    "            1. Compute the global loss\n",
    "            2. Compute the new weights and biases of the third layer (L3)\n",
    "            3. Compute the new weights and biases of the third layer (L2), based on L3\n",
    "            4. Compute the new weights and biases of the third layer (L1), based on L2\n",
    "            5. Return the global LOSS and the updated GRADS\n",
    "        \"\"\"\n",
    "        # y_hat => 10x3\n",
    "        y_hat = predictions\n",
    "        # size of the training set\n",
    "        N = y_hat.size\n",
    "        \n",
    "        # change the pandas Series to a numpy ndarray\n",
    "        # 10x3\n",
    "        y = pd.get_dummies(y).values\n",
    "        \n",
    "        if y.shape[1] < 3:\n",
    "            y = np.append(y, np.zeros((y.shape[0], 3 - y.shape[1]), dtype=int), axis=1)\n",
    "        \n",
    "        # derivative of activation function\n",
    "        deriv = self.activation_fn.backward\n",
    "       \n",
    "        # cross entropy function regularized\n",
    "        cross_entropy = np.nan_to_num(-np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / N)\n",
    "        l2 = (self.weight_decay / (2*N)) * np.sum(np.square(self.params['w3']))\n",
    "        loss = np.nan_to_num(cross_entropy + l2)\n",
    "        \n",
    "        # 10x3 - 10x3 = 10x3\n",
    "        l4_d = y_hat - y\n",
    "        \n",
    "        # 10x3 dot 3x10 * 10x10\n",
    "        l3_d = self.params['w3'].dot(l4_d.T) * deriv(self.cache['a2'])\n",
    "        # 10x10 dot 10x10 * 10x10\n",
    "        l2_d = self.params['w2'].dot(l3_d) * deriv(self.cache['a1'])\n",
    "        \n",
    "        # Real\n",
    "        self.grads['b3'] = cross_entropy\n",
    "        # 10x3\n",
    "        self.grads['w3'] = np.nan_to_num(cross_entropy + (self.weight_decay * self.params['w3']))\n",
    "        \n",
    "        # 10x?\n",
    "        self.grads['b2'] = np.sum(l3_d, axis=0)\n",
    "        # 10x10\n",
    "        self.grads['w2'] = np.nan_to_num(np.sum(l3_d, axis=0) + (self.weight_decay * self.params['w2']))\n",
    "        \n",
    "        # 10x?\n",
    "        self.grads['b1'] = np.sum(l2_d, axis=0)\n",
    "        # 4x10\n",
    "        self.grads['w1'] = np.sum(l2_d, axis=0)\n",
    "        \n",
    "        return loss, self.grads\n",
    "        \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Faz um step do gradiente descendente, nao alterar\n",
    "        \"\"\"\n",
    "        self.params[\"w3\"] = self.params[\"w3\"] - self.learning_rate*(self.grads['w3'])\n",
    "        self.params[\"b3\"] = self.params[\"b3\"] - self.learning_rate*(self.grads['b3'])\n",
    "        \n",
    "        self.params[\"w2\"] = self.params[\"w2\"] - self.learning_rate*(self.grads['w2'])\n",
    "        self.params[\"b2\"] = self.params[\"b2\"] - self.learning_rate*(self.grads['b2'])\n",
    "        \n",
    "        self.params[\"w1\"] = self.params[\"w1\"] - self.learning_rate*(self.grads['w1'])\n",
    "        self.params[\"b1\"] = self.params[\"b1\"] - self.learning_rate*(self.grads['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devem ser alterados os hiperparâmetros para melhor otimização da rede\n",
    "## Alguns dos hiperparâmetros que podem ser modificados estão na célula abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "activation_fn = 'relu'\n",
    "weight_decay = 0.001 # lambda\n",
    "epochs = 1000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.loc[:,\"target\"].unique() # classes do dataset: n_classes = 3\n",
    "classes.sort()\n",
    "\n",
    "network = NeuralNetwork(df.shape[1]-1, 10, 10, output_size=len(classes), learning_rate=learning_rate,\n",
    "                        activation_fn=activation_fn, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Nao alterar esta celula\"\"\"\n",
    "# Cria o grafico e atualiza os pesos\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8,3))\n",
    "ax1.set_title('Training Loss')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax3.set_title('Validation Accuracy')\n",
    "\n",
    "t = tqdm(range(epochs))\n",
    "losses = []\n",
    "training_accuracy = []\n",
    "accuracy = []\n",
    "\n",
    "# Visualizacao do treinamento\n",
    "drawn, = ax1.plot(np.arange(0), losses, c='r')\n",
    "drawn2, = ax2.plot(np.arange(0), training_accuracy, c='r')\n",
    "drawn3, = ax3.plot(np.arange(0), accuracy, c='r')\n",
    "\n",
    "# Loop de treinamento\n",
    "for e in t:\n",
    "    # aleatoriza o treino e zera estatisticas\n",
    "    df = df.sample(frac=1., random_state=SEED)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Loop da epoca\n",
    "    # range from 0 to df.shape[0]-batch_size, step = batch_size\n",
    "    for i in range(0, df.shape[0]-batch_size, batch_size):\n",
    "        total += batch_size\n",
    "        batch = df.iloc[i:(i+batch_size), :]\n",
    "        y = df.iloc[i:(i+batch_size)]\n",
    "        y = y.loc[:, \"target\"]\n",
    "        batch = batch.drop([\"target\"], axis=1)\n",
    "        predictions = network.forward(batch.values)\n",
    "        loss, grads = network.backward(y)\n",
    "        total_loss += loss\n",
    "        network.optimize()\n",
    "        correct += sum(y.values == predictions.argmax(axis=1))\n",
    "    \n",
    "    # Atualiza estatisticas e graficos\n",
    "    training_accuracy.append(correct/total)\n",
    "    losses.append(total_loss)\n",
    "    t.set_description('Loss: %.3f' % total_loss)\n",
    "    drawn.set_data((np.arange(len(losses)), losses))\n",
    "    ax1.relim()\n",
    "    ax1.set_xlim((0, e))\n",
    "    ax1.autoscale_view()\n",
    "    drawn2.set_data((np.arange(len(training_accuracy)), training_accuracy))\n",
    "    ax2.relim()\n",
    "    ax2.set_xlim((0, e))\n",
    "    ax2.autoscale_view()\n",
    "    \n",
    "    ### VALIDACAO ###\n",
    "    \n",
    "    batch = df_val\n",
    "    y = df_val.loc[:, \"target\"]\n",
    "    batch = batch.drop([\"target\"], axis=1)\n",
    "    predictions = network.forward(batch.values)\n",
    "    correct = np.sum(y.values == predictions.argmax(axis=1))\n",
    "    accuracy.append(correct/df_val.shape[0])\n",
    "    \n",
    "    # Atualiza grafico de validacao\n",
    "    drawn3.set_data((np.arange(len(accuracy)), accuracy))\n",
    "    ax3.relim()\n",
    "    ax3.set_xlim((0, e))\n",
    "    ax3.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
